# House Price Predictor - Project Log

## Project Overview
Building a house price prediction model from scratch using linear regression and its variants.
- **Dataset**: Kaggle House Prices (1458 samples, 81 features)
- **Goal**: Implement ML algorithms from scratch, understand fundamentals
- **Timeline**: 2 weeks (Days 1-6 complete)

---

## Day 1-2: Data Preparation & EDA
**Date**: January 8-9, 2026

### Tasks Completed
- ‚úÖ Loaded and explored dataset (1460 samples, 81 features)
- ‚úÖ Missing value analysis (19 features with missing data)
- ‚úÖ Correlation analysis with SalePrice
- ‚úÖ Feature selection: 7 features chosen based on correlation
  - OverallQual (0.79), GrLivArea (0.71), GarageCars (0.64)
  - TotalBsmtSF (0.61), 1stFlrSF (0.61), FullBath (0.56), YearBuilt (0.52)
- ‚úÖ Outlier detection and removal (2 outliers removed)
- ‚úÖ Log transformation applied to SalePrice (skewness: 1.88 ‚Üí 0.12)
- ‚úÖ Train/Val/Test split: 75%/15%/10% (1092/220/146 samples)
- ‚úÖ Feature standardization (mean=0, std=1)

### Key Findings
- Right-skewed SalePrice distribution required log transformation
- Strong multicollinearity between some features (e.g., GarageArea vs GarageCars)
- 2 extreme outliers identified (large houses with unusually low prices)

### Files
- `notebooks/01_eda.ipynb`
- `data/train.csv` (gitignored)

---

## Day 3: Linear Regression from Scratch
**Date**: January 10, 2026

### Tasks Completed
- ‚úÖ Implemented `LinearRegressionScratch` class with gradient descent
- ‚úÖ Training: lr=0.01, 1000 iterations
- ‚úÖ Cost reduction: 99.98% (converged successfully)
- ‚úÖ Model evaluation and visualization

### Results
```
Training R¬≤:   0.8449
Validation R¬≤: 0.8484
Gap:           0.0035
```
- ‚úÖ Excellent generalization - no overfitting
- ‚úÖ Validation performs slightly better than training

### Implementation Details
- Forward pass: `y_pred = X @ weights + bias`
- Cost function: MSE = `(1/2m) * sum((y_pred - y)^2)`
- Gradients: `dw = (1/m) * X.T @ (y_pred - y)`, `db = (1/m) * sum(y_pred - y)`
- Update rule: `weights -= lr * dw`, `bias -= lr * db`

### Files
- `src/linear_regression.py` (LinearRegressionScratch class)

---

## Day 4: Polynomial Regression
**Date**: January 11, 2026

### Tasks Completed
- ‚úÖ Implemented `PolynomialRegressionScratch` (Normal Equation)
- ‚úÖ Implemented `PolynomialRegressionGradientDescent` (GD with lr=0.06, 2000 iter)
- ‚úÖ Debugged gradient descent for high-dimensional features (35 features)
- ‚úÖ Tested polynomial degrees 1-5
- ‚úÖ BIC/AIC analysis for model selection
- ‚úÖ Comparative visualizations

### Results
| Degree | Features | BIC      | AIC      | Val R¬≤  |
|--------|----------|----------|----------|---------|
| 1      | 7        | -4023.50 | -4063.46 | 0.8484  |
| 2      | 35       | -3920.13 | -4099.98 | 0.8420  |
| 3      | 119      | -3452.07 | -4051.56 | 0.8333  |
| 4+     | 329+     | Exploded | Exploded | Negative|

### Key Learnings
- Polynomial features don't improve performance for this dataset
- Linear model (degree=1) is optimal by all metrics
- High-degree polynomials (4+) catastrophically overfit
- Gradient descent required careful learning rate tuning (lr=0.06 for degree=2)
- Normal Equation: Œ∏ = (X^T X)^-1 X^T y (instant, exact solution)

### Challenges Overcome
- Initial gradient descent diverged with lr=0.01 for 35 features
- Tested learning rates from 0.001 to 0.1
- Found optimal lr=0.06 for degree=2 polynomial (2000 iterations)
- Both implementations (Normal Equation & GD) matched sklearn perfectly

### Files
- `src/linear_regression.py` (added PolynomialRegressionScratch, PolynomialRegressionGradientDescent)

---

## Day 5: Regularization (Ridge & Lasso)
**Date**: January 12, 2026

### Tasks Completed
- ‚úÖ Implemented `RidgeRegressionScratch` (L2 regularization, Normal Equation)
- ‚úÖ Implemented `LassoRegressionScratch` (L1 regularization, Coordinate Descent)
- ‚úÖ Tested multiple alpha values for both methods
- ‚úÖ Feature selection analysis with Lasso
- ‚úÖ Comparative visualizations

### Ridge Results
- Tested alphas: [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
- Best alpha: 10.0
- Training R¬≤: 0.8450
- Validation R¬≤: 0.8476
- Ridge formula: Œ∏ = (X^T X + Œ±I)^-1 X^T y
- No improvement over linear model

### Lasso Results
- Tested alphas: [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]
- Best alpha: 0.0001
- Training R¬≤: 0.8451
- Validation R¬≤: 0.8475
- Features: 7/7 (no feature elimination until alpha=10)
- Converged in ~18 iterations using coordinate descent
- Soft thresholding operator for L1 penalty

### Key Findings
- **Regularization doesn't help** - model is not overfitting
- Linear model with 7 features is already well-regularized
- Ridge and Lasso performance nearly identical to baseline
- **Conclusion**: Simple linear regression is optimal for this dataset

### Implementation Details
- Ridge: Added Œ±I penalty to normal equation
- Lasso: Coordinate descent with soft thresholding
- Lasso converged much faster than expected (18 iterations vs 1000 max)

### Files
- `src/linear_regression.py` (added RidgeRegressionScratch, LassoRegressionScratch)

---

## Day 6: Model Validation & Deep Analysis
**Date**: January 13, 2026

### Part 1: sklearn Comparison & Validation (1.5 hrs)

#### Tasks Completed
- ‚úÖ Validated all 5 implementations against sklearn
- ‚úÖ K-fold cross-validation (5-fold and 10-fold)
- ‚úÖ **Final test set evaluation**

#### sklearn Validation Results
| Model               | Our Val R¬≤ | sklearn Val R¬≤ | Difference | Match |
|---------------------|------------|----------------|------------|-------|
| Linear Regression   | 0.8484     | 0.8475         | 0.0009     | ‚úì     |
| Polynomial (deg=2)  | 0.8420     | 0.8420         | 0.0000     | ‚úì     |
| Ridge (Œ±=10)        | 0.8476     | 0.8476         | 0.0000     | ‚úì     |
| Lasso (Œ±=0.01)      | 0.8475     | 0.8423         | 0.0052     | ‚úì*    |

*Lasso within acceptable range (<0.01) - difference due to solver variations

#### Cross-Validation Results
**5-fold CV:**
- Linear: 0.8407 ¬± 0.0821
- Polynomial: 0.8407 ¬± 0.0688
- Ridge: 0.8406 ¬± 0.0828
- Lasso: 0.8387 ¬± 0.0827

**10-fold CV:**
- Linear: 0.8389 ¬± 0.1138
- Higher variance with smaller folds
- Fold 7 problematic (R¬≤=0.680) across all models

#### Test Set Evaluation (FINAL)
```
Test Set Performance:
  R¬≤ Score:  0.8714
  RMSE:      0.1487 (log scale)
  MAE:       0.1045 (log scale)
  
Comparison:
  Train R¬≤:  0.8459
  Test R¬≤:   0.8714  ‚Üê Better than training!
  CV Mean:   0.8389
```

**Key Finding:** Test set performed BETTER than training - unusual but validates strong generalization.

---

### Part 2: Error Analysis (2 hrs)

#### Residual Analysis
- Mean residual: -0.0073 (nearly unbiased)
- Std: 0.1485
- Min/Max: -0.6832 / 0.5002
- Distribution: Nearly normal with slight right skew
- Q-Q plot: Tails deviate slightly (outliers at extremes)
- Homoscedastic: Constant variance across predictions ‚úì

#### Worst 10 Predictions Analysis
- Worst error: House #134 (actual=$35,311, predicted=$69,922, error=-49.5%)
- 7 out of 10 worst are over-predictions
- 3 out of 10 worst are under-predictions

#### Pattern Identified: **Model Struggles with Low-Quality Houses**
Worst predictions vs all test houses:
- OverallQual: 5.2 vs 6.1 (-15.2%)
- GarageCars: 1.5 vs 1.8 (-15.8%)
- **FullBath: 1.1 vs 1.5 (-27.3%)**
- YearBuilt: 1946 vs 1973 (-1.4%)

**Conclusion:** Linear model breaks down for distressed/low-end properties.

---

### Part 3: Feature Insights (1.5 hrs)

#### Feature Importance (by absolute weight)
1. **GrLivArea**: 0.1494 (29.8% of total contribution)
2. **OverallQual**: 0.1288 (24.3%)
3. **YearBuilt**: 0.0864 (17.1%)
4. **TotalBsmtSF**: 0.0581 (10.6%)
5. **GarageCars**: 0.0475 (8.6%)
6. **1stFlrSF**: 0.0252 (5.1%)
7. **FullBath**: -0.0207 (4.5%) ‚Üê NEGATIVE!

#### Why is FullBath Negative?
- Correlation with GrLivArea: 0.64
- **Multicollinearity artifact**
- When controlling for size, more bathrooms = less space for other rooms
- Not causal - just mathematical consequence

#### Sensitivity Analysis (Price Impact of +1 Std Dev)
| Feature      | Price Change | % Change |
|--------------|--------------|----------|
| GrLivArea    | +$26,897     | +16.1%   |
| OverallQual  | +$22,951     | +13.7%   |
| YearBuilt    | +$15,074     | +9.0%    |
| TotalBsmtSF  | +$9,984      | +6.0%    |
| GarageCars   | +$8,129      | +4.9%    |
| 1stFlrSF     | +$4,260      | +2.6%    |
| FullBath     | -$3,416      | -2.0%    |

---

### Part 4: Model Interpretability (1 hr)

#### Final Model Equation (Log Scale)
```
log(Price) = 12.0278
           + 0.1288 √ó OverallQual
           + 0.1494 √ó GrLivArea
           + 0.0475 √ó GarageCars
           + 0.0581 √ó TotalBsmtSF
           + 0.0252 √ó 1stFlrSF
           - 0.0207 √ó FullBath
           + 0.0864 √ó YearBuilt
```

#### Percentage Impact (Per 1 Std Dev Increase)
- GrLivArea ‚Üí +16.11%
- OverallQual ‚Üí +13.75%
- YearBuilt ‚Üí +9.03%
- TotalBsmtSF ‚Üí +5.98%
- GarageCars ‚Üí +4.87%
- 1stFlrSF ‚Üí +2.55%
- FullBath ‚Üí -2.05%

#### Case Study: Cheapest vs Most Expensive
**Cheapest House (#134):**
- Actual: $35,311
- Predicted: $69,922 (error: -49.5%)
- Features: Quality=2, Size=480 sqft, 0 bathrooms, built 1949
- **Model over-predicted** - linear assumption fails for distressed properties

**Most Expensive (#130):**
- Actual: $451,950
- Predicted: $409,271 (error: +9.4%)
- Features: Quality=10, Size=2296 sqft, 3 garage, built 2008
- **Model accurate** - linear assumption holds for luxury properties

---

### Part 5: Summary & Recommendations

#### üìä Final Model Performance
```
Training R¬≤:       0.8459
Cross-Val R¬≤:      0.8389 (¬±0.0569)
Test R¬≤:           0.8714  ‚Üê BEST
Test RMSE:         0.1487 (log scale)
Test MAE:          0.1045 (log scale)
```

#### üéØ Key Findings
1. Linear regression is optimal (polynomial/regularization didn't help)
2. Model explains 87% of price variance on test set
3. Most important features: GrLivArea (16%), OverallQual (14%)
4. Model struggles with low-quality houses (Q1 properties)
5. All implementations matched sklearn (validated!)

#### ‚ö†Ô∏è Limitations
1. FullBath has negative weight (multicollinearity with GrLivArea)
2. Overpredicts cheap houses (< $50k) by ~50%
3. Linear assumption breaks down for distressed properties
4. Missing interaction effects (e.g., Quality √ó Size)
5. Fold 7 in CV consistently underperforms (R¬≤=0.680)

#### üí° Recommendations for Improvement
1. Remove FullBath or combine with HalfBath feature
2. Add interaction term: OverallQual √ó GrLivArea
3. Consider separate models for luxury vs budget homes
4. Investigate outlier houses in CV fold 7
5. Try ensemble methods (Random Forest, Gradient Boosting)

### Files
- `notebooks/01_eda.ipynb` (updated with Day 6 analysis)
- All visualizations saved

---

## Summary: Days 1-6

### Best Model: Linear Regression
```
Final Performance:
  Test R¬≤:    0.8714
  Train R¬≤:   0.8459
  CV R¬≤:      0.8389
  Features:   7
  Method:     Gradient Descent
```

### All Models Comparison
| Model               | Train R¬≤ | Val R¬≤  | Test R¬≤ | Notes                    |
|---------------------|----------|---------|---------|--------------------------|
| Linear Regression   | 0.8449   | 0.8484  | 0.8714  | ‚úÖ Best - Simple & Effective |
| Polynomial (deg=2)  | 0.8575   | 0.8420  | N/A     | Slight overfitting       |
| Ridge (Œ±=10)        | 0.8450   | 0.8476  | N/A     | No improvement           |
| Lasso (Œ±=0.0001)    | 0.8451   | 0.8475  | N/A     | No improvement           |

### Project Achievements
1. ‚úÖ Built 5 ML algorithms from scratch (all matched sklearn)
2. ‚úÖ Comprehensive validation (train/val/test + CV)
3. ‚úÖ Deep error analysis (identified failure modes)
4. ‚úÖ Full interpretability (feature importance, sensitivity analysis)
5. ‚úÖ Production-ready insights (actionable recommendations)

### Key Learnings
1. **Simplicity wins**: Linear model outperforms complex variants
2. **Feature engineering > model complexity**: 7 features beat 35 polynomial features
3. **Validation is crucial**: Test set revealed true performance (0.8714)
4. **Error analysis reveals blind spots**: Model struggles with low-end properties
5. **Interpretability matters**: Understanding weights leads to actionable insights
6. **Occam's Razor validated**: Simpler model is better when performance is equal

---

## Next Steps (Days 7-14)

### Day 7-9: Documentation & Article
- [ ] Code refactoring and comprehensive documentation
- [ ] Write Medium article explaining methodology
- [ ] Create publication-quality visualizations
- [ ] GitHub README update with results

### Day 10-12: Advanced Improvements
- [ ] Implement interaction features (Quality √ó Size)
- [ ] Build ensemble model (Random Forest baseline)
- [ ] Feature engineering: create bathroom ratio, age categories
- [ ] Separate models for price segments

### Day 13-14: Deployment
- [ ] Model serialization (pickle/joblib)
- [ ] Create prediction pipeline
- [ ] Simple Flask/FastAPI demo
- [ ] Docker containerization
- [ ] Deploy to cloud (Heroku/AWS)

---

## Technical Specifications

### Environment
- Python 3.12
- Libraries: NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn, SciPy
- IDE: PyCharm + Jupyter Lab
- Version Control: Git/GitHub

### Repository Structure
```
house-price-predictor/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ train.csv (gitignored)
‚îÇ   ‚îî‚îÄ‚îÄ test.csv (gitignored)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ 01_eda.ipynb (all analysis)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ linear_regression.py (5 classes)
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ PROJECT_LOG.md
```

### Classes Implemented (All Validated ‚úì)
1. `LinearRegressionScratch` - Gradient descent
2. `PolynomialRegressionScratch` - Normal equation
3. `PolynomialRegressionGradientDescent` - GD for polynomials
4. `RidgeRegressionScratch` - L2 regularization
5. `LassoRegressionScratch` - L1 regularization with coordinate descent

---

## Lessons Learned

### Technical
- Gradient descent requires learning rate tuning for high dimensions
- Normal equation is exact but requires matrix inversion (O(n¬≥))
- Feature scaling is CRITICAL for gradient descent convergence
- BIC/AIC excellent for model selection
- Cross-validation reveals stability across data splits
- Test set is the ultimate truth

### ML Engineering
- Validation methodology matters more than model choice
- Error analysis reveals actionable insights
- Interpretability enables better decisions
- Multicollinearity can create counterintuitive weights
- Simple models are easier to debug, deploy, and maintain

### Practical
- Exploratory data analysis is 50% of the work
- Feature selection beats feature creation (for this dataset)
- Outliers have massive impact (2 outliers removed)
- Log transformation critical for skewed targets
- sklearn implementations are gold standard for validation

### Project Management
- Incremental development (day-by-day) builds confidence
- Document as you go (easier than retroactive documentation)
- Visualizations clarify complex patterns
- Git commits preserve progress

---

---

## Day 7: Feature Engineering & Model Optimization
**Date**: January 14, 2026

### Part 1: Feature Engineering (5 hours)

#### Tasks Completed
- ‚úÖ Created `src/feature_engineering.py` module
- ‚úÖ Engineered 8 new features (interaction, ratio, composite)
- ‚úÖ Tested models with 15 features (7 base + 8 engineered)
- ‚úÖ Analyzed feature importance for engineered features
- ‚úÖ Tested selective feature engineering (best 2 only)

#### Engineered Features
1. **Interaction Terms:**
   - Quality_Size = OverallQual √ó GrLivArea
   - Age_Quality = House_Age √ó OverallQual

2. **Ratio Features:**
   - Bath_Density = FullBath / (GrLivArea/1000)
   - Garage_Ratio = GarageCars / GrLivArea
   - Basement_Ratio = TotalBsmtSF / 1stFlrSF

3. **Composite Features:**
   - Total_Space = GrLivArea + TotalBsmtSF
   - House_Age = 2026 - YearBuilt
   - Is_New = Binary flag (built after 2000)

#### Results
| Model Configuration | Features | Test R¬≤ | Change |
|---------------------|----------|---------|--------|
| Base                | 7        | 0.8714  | baseline |
| All Engineered      | 15       | 0.8665  | -0.0049 |
| Selective (best 2)  | 9        | 0.8668  | -0.0045 |

**Key Finding:** Feature engineering decreased performance - base features are already optimal.

#### Feature Importance Analysis
**Top engineered features:**
- Quality_Size: 2nd most important overall (weight=0.066)
- Total_Space: 3rd most important (weight=0.047)
- House_Age: Redundant with YearBuilt (cancels out)
- Bath_Density: Negative weight (amplified multicollinearity)

---

### Part 2: Multicollinearity Resolution (1 hour)

#### Problem Identified
- FullBath has negative weight (-0.0207)
- Correlation with GrLivArea: 0.638
- Counterintuitive: more bathrooms = lower price?

#### Investigation
- Only 9 houses (0.6%) have 0 bathrooms
- FullBath distribution: mostly 1-2 bathrooms
- Multicollinearity confirmed

#### Solution Implemented
**Removed FullBath from feature set**

#### Results
```
Model Comparison:
  With FullBath (7 features):    Test R¬≤ = 0.8714
  Without FullBath (6 features): Test R¬≤ = 0.8718
  Improvement: +0.0005
```

**Final 6 Features:**
1. OverallQual (0.1262)
2. GrLivArea (0.1394) ‚Üê Most important
3. GarageCars (0.0462)
4. TotalBsmtSF (0.0602)
5. 1stFlrSF (0.0245)
6. YearBuilt (0.0810)

**All weights now positive** - multicollinearity resolved ‚úì

---

### Part 3: Cross-Validation Optimization (1.5 hours)

#### Problem Identified: Fold 7 Outlier
**10-Fold CV (no shuffle):**
- Mean R¬≤: 0.8389
- Std: 0.0569 (high variance)
- **Fold 7: R¬≤ = 0.6799** ‚Üê Problematic

#### Investigation
Analyzed Fold 7 characteristics:
- Contains 131 houses
- OverallQual: -0.11 (15.2% below average)
- GrLivArea: -0.09 (smaller houses)
- YearBuilt: -0.06 (older houses)
- Price: $153k avg vs $166k overall

**Root Cause:** Sequential data split created cluster of low-quality houses in Fold 7. Linear model struggles with budget segment.

#### Solution Implemented
**Use shuffled K-Fold instead of sequential split**

#### Results
```
Cross-Validation Comparison:
  No Shuffle:  Mean=0.8389, Std=0.0569, Min=0.6799
  With Shuffle: Mean=0.8423, Std=0.0256, Min=0.7834
  
Improvements:
  - Std reduced by 55%: 0.0569 ‚Üí 0.0256
  - Worst fold improved: 0.6799 ‚Üí 0.7834 (+0.1035)
  - Mean improved: 0.8389 ‚Üí 0.8423 (+0.0034)
```

**Fold 7 problem eliminated** ‚úì

---

### Day 7 Summary

#### Final Model Specifications
```
Model: Linear Regression (Gradient Descent)
Features: 6 (removed FullBath)
Learning Rate: 0.01
Iterations: 1000

Performance:
  Training R¬≤: 0.8447
  CV R¬≤ (10-fold shuffle): 0.8423 ¬± 0.0256
  Test R¬≤: 0.8718

Feature Weights:
  GrLivArea:    0.1394 (most important)
  OverallQual:  0.1262
  YearBuilt:    0.0810
  TotalBsmtSF:  0.0602
  GarageCars:   0.0462
  1stFlrSF:     0.0245
  Bias:        12.0278
```

#### Key Achievements
1. ‚úÖ Feature engineering thoroughly tested (conclusion: base features optimal)
2. ‚úÖ Multicollinearity resolved (removed FullBath, +0.0005 R¬≤)
3. ‚úÖ CV methodology optimized (shuffled K-Fold, 55% std reduction)
4. ‚úÖ All model weights now positive and interpretable
5. ‚úÖ Model stability dramatically improved

#### Key Learnings
1. **More features ‚â† better performance** - engineered features caused overfitting
2. **Domain knowledge matters** - Quality √ó Size interaction made sense but didn't help
3. **Data exploration reveals issues** - fold 7 analysis showed sequential split problem
4. **Multicollinearity diagnosis** - negative weights are red flags
5. **CV methodology critical** - shuffling essential for non-IID data

#### Files Modified
- `notebooks/01_eda.ipynb` (added Day 7 analysis)
- `src/feature_engineering.py` (NEW - feature engineering functions)


---
## Day 8: Elastic Net & Advanced Regularization
**Date**: January 15, 2026

### Tasks Completed
- ‚úÖ Implemented ElasticNetScratch (L1 + L2 regularization)
- ‚úÖ Tested l1_ratio: 0.0 to 1.0 (Ridge ‚Üí Lasso spectrum)
- ‚úÖ Tested alpha: 0.001 to 1.0
- ‚úÖ Grid search on 15 engineered features

### Results
- All configurations: Val R¬≤ ‚âà 0.8479
- No improvement over base linear model
- Elastic Net unable to handle multicollinearity better than feature removal

### Key Finding
**Elastic Net unnecessary** - no overfitting exists in base model. Feature removal remains superior approach to handling multicollinearity.

---

## Day 9: Tree-Based Models & Hyperparameter Tuning
**Date**: January 15, 2026

### Tasks Completed
- ‚úÖ Trained Random Forest (grid search: 72 configs)
- ‚úÖ Trained XGBoost (grid search: optimized params)
- ‚úÖ Compared against linear baseline

### Results
| Model | Test R¬≤ | Overfit | Status |
|-------|---------|---------|--------|
| Random Forest | 0.8594 | 0.0785 | ‚ùå Eliminated |
| XGBoost | 0.8740 | 0.0063 | ü•á New Champion |
| Linear | 0.8718 | 0.0271 | ü•à Silver |

### XGBoost Breakthrough
- **Best params**: n_estimators=300, max_depth=4, lr=0.05, subsample=0.8
- **Fixes linear's failures**: 7 houses where linear failed by 20%+
- **Feature importance**: OverallQual dominates (49%)
- **Captures non-linearity**: Price discontinuities at market segments

### Why XGBoost Won
- Better regularization (0.63% overfit vs RF's 7.85%)
- Gradient boosting > bagging for this dataset
- Handles outliers without overfitting


---


## Day 10: Ensemble Stacking & Production Calibration
**Date**: January 15, 2026

### Part 1: Ensemble Stacking (2 hours)

#### Architecture
- Base Model 1: Linear Regression (6 features)
- Base Model 2: XGBoost (300 estimators)
- Meta-Model: Ridge(Œ±=0.1) blending predictions

#### Blending Weights
- Linear: 0.4123 (41%)
- XGBoost: 0.6593 (66%)
- Bias: -0.8738

#### Results
**Test R¬≤: 0.8833** üèÜ NEW CHAMPION
- Beats XGBoost by +0.0093
- Beats Linear by +0.0115
- RMSE: 0.1416, MAE: 0.0979

#### Victory Analysis
- Ensemble wins on 30.8% of houses (45/146)
- Strategic wins on high-variance outliers
- Averages out extreme errors from both models

---

### Part 2: Confidence Calibration (3 hours)

#### Problem
Initial confidence intervals: 52.7% coverage (too low)

#### Solution: Data-Driven Calibration
Used actual disagreement distribution and residuals to set thresholds:

**Calibrated Thresholds:**
- High confidence: disagreement < 0.0228 (25th percentile)
- Medium: 0.0228 - 0.0673 (25th-75th)
- Low: > 0.0673 (75th+)

**Calibrated Margins:**
- High: ¬±0.1416 (covers 80% of errors)
- Medium: ¬±0.2219 (covers 90%)
- Low: ¬±0.3057 (covers 95%)

#### Final Coverage: 88.4% ‚úÖ
Meets industry standard (>85%)

**Distribution:**
- High confidence: 25.3% of predictions
- Medium: 49.3%
- Low: 25.3%

### Day 10 Summary

#### Key Achievements
1. ‚úÖ Ensemble stacking: +0.93% over XGBoost
2. ‚úÖ Calibrated confidence system (88.4% coverage)
3. ‚úÖ Production-ready model saved to disk
4. ‚úÖ Honest uncertainty quantification

#### System Design Insight
**Avoided switching models based on uncertainty** - ensemble already encodes this mathematically. Instead, used disagreement for confidence scoring (user-facing transparency).

#### Final Model Card
- Architecture: Linear + XGBoost ‚Üí Ridge meta-learner
- Performance: 0.8833 R¬≤ (88.3% variance explained)
- Calibration: 88.4% of predictions fall within stated intervals
- Deployment ready: Serialized with preprocessing pipeline



---

### Part 3: Model Serialization

#### Saved Artifacts
- `models/ensemble_production_v1.pkl` (main)
- `models/linear_model.pkl`
- `models/xgboost_model.pkl`

#### Production Model Specs
##### Final Ensemble Model v1.0
- `Test R¬≤: 0.8833`
- `Coverage: 88.4%`
- `Confidence system: 3 levels (high/med/low)`
- `Features: 6 (OverallQual, GrLivArea, GarageCars, TotalBsmtSF, 1stFlrSF, YearBuilt)`


## Summary: Days 1-10 - PROJECT COMPLETE

### Champion Model: Ensemble (Linear + XGBoost)
#### Final Performance:
- **Test R¬≤:**    0.8833 üèÜ
- **RMSE:**       0.1416 (log scale)
- **MAE:**        0.0979 (log scale)
- **Coverage:**   88.4%
- **Features:**   6


### Evolution Timeline
| Day | Model | Test R¬≤ | Delta |
|-----|-------|---------|-------|
| 3-6 | Linear (from scratch) | 0.8718 | baseline |
| 9 | XGBoost | 0.8740 | +0.0022 |
| 10 | Ensemble + Calibration | 0.8833 | +0.0115 |

### Project Achievements
1. ‚úÖ Built 6 ML algorithms from scratch (all matched sklearn)
2. ‚úÖ Comprehensive validation (train/val/test + CV)
3. ‚úÖ Advanced ensemble stacking
4. ‚úÖ Production-ready confidence system (88.4% coverage)
5. ‚úÖ Model serialization & deployment prep

### Key Learnings - Full Journey
1. **Linear strong baseline**: 87% performance, interpretable
2. **Tree models handle outliers**: XGBoost +0.2% over linear
3. **Ensemble strategic**: Wins on high-variance cases (30.8%)
4. **Calibration critical**: Coverage jumped 52% ‚Üí 88%
5. **System design > algorithms**: Confidence scoring beats model switching
 
---





**Last Updated**: January 15, 2026
**Status**: Days 1-7 Complete ‚úÖ | Ready for Days 7-14
**Next Milestone**: Medium Article + Code Documentation

